\newcommand{\todo}[1]{{\em \textcolor{red}{ #1}}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\lang}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\vep}{\bm{\epsilon}}
\newcommand{\ep}{\epsilon}
\newcommand{\pars}{\vec{\theta}}
\newcommand{\dev}{\mathrm{d}}
\newcommand{\mstar}{h^{-1}M_\odot}
\newcommand{\hst}{\project{HST}}
\newcommand{\wfc}{\project{WFC3}}

\chapter{Super-resolution PSF model of HST WFC3-IR\chaplabel{wfc3ir}}

This \paper is joint work with Ross~Fadely (Insight) and David~W.~Hogg (NYU) and it is being prepared for submission. 

\section{Chapter abstract}

Accurate model of the Point Spread Function is crucial for reliable point source photometry, astrometry, and weak lensing studies.
The PSF model of the $\hst$ $\wfc$ IR channel does not meet the accuracy required by these science goals. In addition, the PSF of the $\hst$ $\wfc$ IR 
channel is poorly sampled. Weak lensing studies of poorly resolved images of faint distant galaxies demands 
a great knowledge of the PSF sampled at a resolution higher than of the $\hst$ $\wfc$ pixels.
In this investigation, we present a generative model of every image taken by the $\wfc$ IR channel as a set of a number of point sources convolved
with the instrument PSF. In particular, we focus on modeling the pixel-convolved PSF, the instrument PSF convolved with the pixel response function. 
In particular, we model the images of point sources observed in the calibration program of the $\hst$ $\wfc$ IR channel in the F160W bandpass. We 
expect the inference of the super-resolution PSF in the other bandpasses of the IR channel to follow a simillar procedure. 
We find that we can find th optimal soution of the problem by using a variance model that correctly takes into account the model 
uncertainty and a regularization term that imposes a smoothness condition on the super-resolution PSF 

\section{Introduction}

The Point Spread Function (hereafter PSF) determines the fraction of photons from a given point source that lands on a particular location 
from the center of that point source on a detector. The PSF of the $\hst$ $\wfc$ camera is extremely undersampled. That is, if a center of an observed star lies 
on the center of a pixel, a significant fraction of the brightness of that star will be 
encapsulated by the detector's pixel that contains the centroid of the star.
In other words, the full-width half-maximum (FWHM) of the PSF is not spanned by multiple pixels since the designed 
pixels are large. This is mainly the result of a compromise made in design of the detectors to cover a wider field of view.

Poor sampling of the PSF by detectors renders many astronomical image processing tasks difficult. Precise astrometry and photometry 
of individual point sources requires knowledge of the PSF sampled on higher resolution than that of the detector pixels. 
In uncalibrated observations, the images of stars are generated by convolution of the point sources with the PSF and multiplication 
with a non-uniform detector sensitivity called the \emph{flat-field}. Flat-field corrections are important for photometry and astrometry of 
individual point sources. In under-sampled images, capturing the sub-pixel variations of the flat-field requires having a higher resolution 
model of the PSF. 

Furthermore, cosmic shear studies require accurate measurement of the shapes of individual distant galaxies. The galaxies used for estimation 
of the cosmic shear signal are faint and barely resolved by the $\hst$ $\wfc$ detectors. In order to reliably estimate the ellipticities of these poorly 
resolved galaxies through model fitting, we need to convolve the model describing the light distribution of these galaxies with a higher resolution PSF model. 
Therfore, having access to a higher resolution model of the PSF is an essential ingredient in weak lensing studies of galaxies detected by the $\hst$ $\wfc$ IR 
channel.   

In this investigation, we focus on modeling the pixel-convolved PSF which is the optical (instrumental) PSF convolved with the 
pixel response function. It is the pixel-convolved PSF that can be directly estimated from observation without making any assumption 
about the sensitivity of detector pixels. Moreover, it is the pixel-convolved PSF that can be used to perform astrometry and photometry measurements as well as 
galaxy shape measurement for weak lensing analyses. All these procedures involve fitting a model to the pixel-convolved PSF. 

Alternatively, one can deliver a model of the optical (instrumental) PSF based on physical models of the telescope optics. 
This involves measurement of the optical wavefront at all locations across the focal plane and use of physically motivated parameters 
to model the image changes in wavefront. Significant progress has been made in exploring the space of physical models that can efficiently 
describe the optical PSF \citep{zernike,krist1995,krist2011,galsim_software}. 

Development of the optical models of the PSF are valuable efforts for simulations, validations, and understanding the systematic uncertainties 
that can affect astronomical inferences \citep{great3,galsim}. In practice however, using the optical PSF for photometry, astrometry and weak lensing studies 
requires fitting a star's model to the wavefront PSF model. The major drawbacks of this approach are the following: $(i)$ it does not make use of the observed 
data, and ${ii}$ it makes strong assumptions about sensitivity of detector pixels when it is used for model fitting. Since these reasons, we focus on building a 
data driven empirical model of the PSF. From now on in this \paper, we refer to the pixel-convolved PSF as the PSF. 


This \paper is structured as follows: In Section \ref{sec:data} we discuss the observations and data reduction. In Section \ref{sec:method} we discuss the algorithm we 
have developed for infering the super-resolution PSF model of the $\hst$ $\wfc$ IR channel observations. In Section \ref{sec:results} we present the preliminary results, and 
in Section{sec:summary} we summarize and conclude. 

\section{Data}\label{sec:data}

The $\wfc$ IR camera consists of four filters: F105W, F125W, F140W, F160W.
In what follows in the rest of this \paper\ we focus on modeling the images of point sources in the F160W filter of $\hst$ $\wfc$ IR channel. 
In our analysis, we make use of the FLT images that our calibrated with the recent models of charge transfer efficiency, flat-field, etc. 
We choose not use Drizzeled images \citep{drizzle,astrodrizzle} because they introduce correlated noise and 
distortions to the PSF that are not consistent across the image (a new algorithm \citep{olic} developed for the WFIRST mission does not suffer from these issues). 
This poses a challenge for optimal extraction of information and accurate measurement of shapes. Therefore the pixels in the FLT images are better-suited for performing PSF modeling. 

For each FLT image, we follow the following procedure to select stars with sufficiently high signal-to-noise ratio 
for PSF mdoeling. First the Source Extractor software \citep{sextractor} is run on each image to detect the objects. 
For each object, the stellarity index and blending/error flags are extracted. An object is called a \emph{star} if no blending/error flag is 
raised, its stellarity index is greater than 0.8 and if its peak pixel brightness values is 25 times greater than the median pixel brightness 
value.  

The FLT files obtained from the HST MAST archive are accompanied by Data Quality extension files. While extracting the super-resolution PSF model 
from the point sources in FLT images, pixels with data quality other than zero are not included in the model. This requirement guarantees that 
the damaged pixels do not contribute to the PSF model. For each point source, we extract a 25 pixels $\time$ 25 pixels patch that is centered on the brightest 
pixel of the point source.   

\section{Method}\label{sec:method}

\subsection{generative model}

We build our empirical model of the PSF by assuming that a given point source at the center of a patch can be described by 
the following quantities: 

\begin{itemize}
\item A global solution for the super-resolution PSF $X$ that is shared between all point sources across the field of view. We assume 
that $X$ is normalized to one and it is centered at the center of the central pixel of the patch. This super-resolution PSF is sampled on 
a grid with a higher resolution that the native pixel grid of $\hst$ $\wfc$ IR observations.

\item Centroid coordinate of a point source at a given patch $\Delta_n = (\Delta x_n, \Delta y_n)$. 
These two parameters dictate the offset between the position of the centroid of the point source $n$ with 
respect to the center of the patch.

\item Flux quantity $f_n$ which is the brightness of the point source.

\item A background quantity $b_n$ which sets the brightness level of the background sky.

\end{itemize}

Given these ingredients, we write down the generative forward model of the point sources in the center of patches in the following way:

\begin{eqnarray}
\mathbf{d}_{n} &=&  \mathbf{m}_{n} + \mathrm{noise}, \\
m_{n,i} &=& f_{n}K^{(n)}_{il} (\Delta_n) X_{l} + b_{n}. 
\label{eq:model}
\end{eqnarray} 

In equation~\ref{eq:model}, $\mathbf{d}_{n}$ is the observed 625 dimensional vector in patch $n$. $\mathbf{m}_{n}$ is the model of patch $n$ with the same dimensionality. The 
noise term \mathrm{noise} has a variance that we present shortly.
The operator $K^{(n)}(\Delta_n)$ is a linear operator that maps the super-resolution psf $X$ to a downsampled PSF at the native 
data grid. That is, when this operator is applied to $X$, it samples the $X$ on a downsampled grid (with the same resolution as the observations) 
that is shifted with respect to the grid on which $X$ is defined. The shift between the two grids is given by the vector $\Delta_n = (\Delta x_n , \Delta y_n)$.


We want to be able to explicitly construct these matrices so we can analytically compute the likelihood function and its derivative with respect to the super-resolution PSF $X$. 
We implement the linear sampling operator $K$ with bivariate cubic spline interpolation. We comparing our implementation of cubic-spline with that of the open source 
software $\mathtt{scipy}$ we note that they have consistent performances.

It is worth noting that there are many possibilities for designing the sampling operator $K$.  The simplest and fastest approach that one could 
choose is bilinear interpolation. But after applying this method to simulated PSFs, we find that this method lacks the sufficient accuracy we 
need for our forward model. Ideally, one could use a Sinc interpolation \citep{bickerton,galsim} which is an optimal choice for preserving information.
But we find sinc interpolation to be computationally very demanding for our purposes. Furthermore, we experimented with Gaussian Process interpolation for designing the downsampling matrix $K$. We found the Gaussian Process interpolation less advantageous than cubic-spline interpolation as it requires additional computing time for setting the hyper parameters of the kernel used in Gaussian Process.

\subsection{likelihood optimization}

Now that we have explained the quantities needed to quantify the generative model of $\wfc$ IR observations, we 
present our strategy for estimating the values of the parameters of individual point sources $\{f_{n},b_{n},\Delta_{n}\}_{n=1}^{N}$ and 
the global solution of the super-resolution PSF $X$.

The factor by which the PSF is undersampled varies from one filter to another. Our strategy for inferring the super-resolution of the PSF model of F160W filter is as follows. 
First we run the Source Extractor algorithm to identify all the stellar sources in the observed data. In this \paper\ we estimate $X$ on a 75 pixels $\time$ 75 pixels grid, which 
has a resolution three times that of the native pixel grid of the observations.

In order to estimate the PSF, we optimize the following likelihood function:

\begin{eqnarray}
-2 \ln L &=& -2 \sum_{n=1}^{N_{patches}} \ln L_{n} \label{eq:total} , \\
-2 \ln L_{n} &=& \sum_{i=1}^{N_{\rm pix}} \Big( \frac{\big(d_{n,i} - m_{n,i} \big)^{2}}{s^{2}_{n,i}} + \ln(2 \pi s_{n,i}^{2}) \Big) \label{eq:patch},
\end{eqnarray}
where $L_{n}$ is the likelihood of patch $n$, $L$ is the likelihood of the entire astronomical image, $s_{n,i}$ is the $i$th component of the 625 dimensional vector $\mathbf{s}_n$, and 
$\mathbf{s}_n^{2}$ is variance of the noise model in patch $n$. The variance is comprised of two terms, one arising from the Gaussian readout noise and another arising from the Poisson noise.
Therefore the total variance is given by a constant per-pixel variance of a Gaussian noise and a gain-like term that is proportional to the model $\mathbf{m}_{n}$:
\beq
\mathbf{s}_n^{2} = \sigma^{2} + g.\mathbf{m}_{n},
\eeq
where we extract Gaussian variance $\sigma^{2}=0.01$ and gain $g=0.05$ from the $\hst$ data. 

It is important to note that our forward model of the $\wfc$ IR 
observations given by euation ~\ref{eq:model} suffers from degeneracies. The first degeneracy is between 
the flux values $\{f_n\}$ and the super-resolution PSF $X$. This degeneracy will prevent us from finding a unique 
solution for $\{f_n\}$ and $X$. This degeneracy can be lifted by adding a reqularization term to equation \ref{eq:total} that imposes 
smoothness condition to $X$. 

We add the following term to the log-likelihood $L$
\beq
C_{\rm reg} &=& -2\ln L + \epsilon \sum_{j,k} \delta^{2}_{j,k},
\label{eq:reg}
\eeq
where the second term is the sum of the squared of the matrix $\delta$ whose $j,k$ components are given by the difference between the $j$th row and $k$th column of the super-resolution PSF 
and its nearest pixels. The prefactor $\epsilon$ sets the strength of the regularization term. This enforces our prior belief that the PSF should be a smoothly varying object.

The second type of degeneracy is the between the centroid offsets $\{\Delta_n\}$ and the super-resolution PSF $X$. In order to lift this degeneracy, we do not add any regularization 
term. Instead, throughout the optimization procedure we enforce a set of conditions that breaks will break this degeneracy. These conditions conditions enforce the super-resolution 
PSF to be normalized to one and centered around the central pixel of the higher resolution grid. 

For inference of the PSF, we follow an optimization procedure simillar to the wellknown method of Expectation Maximization. In the context of astronomy, this 
method has been employed in extreme deconvolution \citep{xd} and eteroscedastic matrix factorization \citep{hmf}. Finding the optimal solution of 
$X$ and $\{f_n , b_n , \Delta_n\}$ is done as follows. 

First we initialize the parameters $X$ and $\{f_n , b_n , \Delta_n\}$. We initialize the centroid offsets ${\Delta_n}$ by the matched-filter polynomial centroiding 
described in Chapter 1. In particular, we correlate a Gaussian kernel with FWHM of 2.0 pixels with the 5 $\times$ 5 central pixels of each patch before applying 
the polynomial centroiding method. The reason for why we apply the polynomial method to the central pixels of each patch is that many of the stars in our $\hst$ observations 
are in crowded fields and therefore applying the matched-filter centroiding method to the entire patch could lead to biases as a result of light contamination from overlapping 
or nearby sources. 

The initial background values $\{b_n\}$ are estimated by taking the median pixel value in each patch. For initializing the flux values $\{f_n\}$, first we subtract the initial 
estimates of background values from each patch. Afterwards, the $\{f_n\}$ values are initialized by computing the sum of the pixel values within 5$\time$5 apertures centered on 
the central pixels of the patches. The reason for limiting our initialization of the flux estimates to the 5$\time$5 apertures is to prevent light contamination of fluxes from 
overlapping and nearby sources. Besides since the PSF is poorly sampled, we expect the majority of the flux values to be captured by a few pixels in the centers of the patches. 

Lastly, the super-resolution PSF $X$ is initialized in the following way. First we subtract our intial estimates of the sky background levels from the pacthes. Then we divid the 
pixel values of the patches by their corresponding initial flux estimates. Then we use the cubic-spline interpolation method to interpolate the rescaled patches to a high resolution 
75 $\time$ 75 grid whose centers is shifted by $\{-\Delta_n\}$ with respect to the initial centroiding estimates. Afterwards, we evaluate the mean of the interpolated rescaled patches 
to get estimate the initial PSF. Furthermore, we normalize the PSF to one and interpolate it such that it is centered on the central pixel of the 75 $\time$ 75 high resolution grid.  

Now that we have initialzed the parameters of our model, we present the EM algorithm for optimizing them. At every iteration, we follow this 
iterative scheme. In the $n$th patch, we update $b_n$ by optimizing $L_n$ while holding $\{f_n, \Delta_n, X\}$ fixed. Then 
we update $f_n$ by optimizing $L_n$ while holding $\{b_n, \Delta_n, X\}$ fixed. Then we update $\Delta_n$ by holding $\{f_n, b_n, X\}$ fixed. 
Once $\Delta_n$ is updated, we update the sampling matrix $K^{(n)}$ accordingly. 

Once $\{b_n, f_n, \Delta_n\}$ for all patches, we update $X$ 
by optimizing the regularized cost function $C_{\rm reg}$ given in equation (\ref{eq:reg}). At every iteration, we normalize the super-resolution PSF and 
interpolate it such that it is centered on the center of the 75 $\time$ 75 grid. We repeat these iterations until the value of regularized cost function 
$C_{\rm reg}$ converges.

Note that we need to set the strength of the smoothness regularization term $\epsilon$. In order to so, first we randomly divide the patches into to sets: 
the training set and the cross validation set.  




%We follow something like \citep{xd,hmf} heteroscedastic matrix factorization ....
%Simillar methods in the literature that rely on compressed sensing \citep{ngole,ngole2}


Ideally one would use the bright isolated sources to estimate the PSF. A considerable fraction of the observed stars are present in the crowded fields. 
In order to alleviate the issue arising from the contamination of stellar patches by the light coming from overlapping sources, we add another term to the noise model which accounts for the discrepancy between the downsampled PSF model on the data grid and the observed star. 

\beq
s_n^2 = \sigma_{n}^{2} + g.m_{n} + qm_{n}^{2}
\eeq 


\begin{algorithm} 
\caption{The procedure for Inferring the Super-Resolution PSF}
\begin{algorithmic}[1] \label{alg:abcpmc}
%\STATE \DATA: D
%\STATE \RESULT: ABC posterior sample of $\pars$
\IF{$t=1:$}
%\STATE $\epsilon_t \gets \infty$
\FOR{$n=1,...,N$}
   \STATE // \emph{This can now be done in parallel for all i}
   %\WHILE{$\rho(X,D)>\epsilon_t$}
   \STATE $b_{n}$ \gets \mathrm{median} \; $(y_{n})$
   \STATE $f_{n}$ \gets $\sum_{i=1}^{N_pix} \big(y_{n,i} - b_{n}\big)$
   \STATE \mathrm{Initialize} $\Delta_{n}$ with the 3\time3 polynomial method
   \STATE $X_{n}$ \gets \mathrm{cubic \; spline} $(\Delta_n)[\big(y_{n,i} - b_{n}\big)/f_n]$
\ENDFOR
\STATE $X$ \gets $\sum_{n=1}^{N} X_{n]$
   %\ENDWHILE
\ENDIF
\IF{$t=2,...,N_{it}:$}
\FOR{$i=1,...,N$}
   \STATE // \emph{This loop can now be done in parallel for all i}
   \WHILE{$\rho(X,D)>\epsilon_t$}
   \STATE Draw $\pars^{*}_{t}$ from $\{\pars_{t-1}\}$ with probabilities $\{w_{t-1}\}$
   \STATE $\pars^{*}_{t} \gets K(\pars^{*}_{t},.)$
   \STATE $X = f(\pars^{*}_{t})$
   \ENDWHILE
   \STATE $\pars^{(i)}_{t} \gets \pars^{*}_{t}$
   \STATE $w^{(i)}_{t} \gets \pi(\pars^{(i)}_{t}) / \big(\sum\limits_{j=1}^{N}w_{t-1}^{(i)}K(\pars^{(j)}_{t-1},\pars^{(i)}_{t}) \big)$
\ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}

\section{Preliminary Results}\label{sec:results}

blah ...

\section{Summary and Conclusion}\label{sec:summary}

blah...
